＜k-近傍法（k-NN）＞
from sklearn.neighbors import KNeighborsClassifier

#k-NNインスタンス。今回は3個で多数決。3の値を変更して色々試すと〇
model = KNeighborsClassifier(n_neighbors=3)
#学習モデル構築。引数に訓練データの特徴量と、それに対応したラベル
model.fit(X_train, y_train)

# .scoreで正解率を算出。
print("train score:",model.score(X_train,y_train))
print("test score:",model.score(X_test,y_test))

＜ラムダムフォレスト＞
from sklearn.ensemble import RandomForestClassifier

# 識別モデルの構築
model = RandomForestClassifier(max_depth=30, n_estimators=30, random_state=42)
#学習モデル構築。引数に訓練データの特徴量と、それに対応したラベル
model.fit(X_train, y_train)
# .scoreで正解率を算出。
print("train score:",model.score(X_train,y_train))
print("test score:",model.score(X_test,y_test))

＜決定木＞
from sklearn.tree import DecisionTreeClassifier

# 決定木インスタンス(木の深さ3)
model = DecisionTreeClassifier(max_depth=3)
#学習モデル構築。引数に訓練データの特徴量と、それに対応したラベル
model.fit(X_train, y_train)

# .scoreで正解率を算出。
print("train score:",model.score(X_train,y_train))
print("test score:",model.score(X_test,y_test))

＜サポートベクターマシン（SVM）＞
from sklearn.svm import LinearSVC

# SVMインスタンス
model = LinearSVC()
#学習モデル構築。引数に訓練データの特徴量と、それに対応したラベル
model.fit(X_train, y_train)

# .scoreで正解率を算出。
print("train score:",model.score(X_train,y_train))
print("test score:",model.score(X_test,y_test))


＜ロジスティック回帰＞
from sklearn.linear_model import LogisticRegression

# ロジスティック回帰インスタンス
model = LogisticRegression()
#学習モデル構築。引数に訓練データの特徴量と、それに対応したラベル
model.fit(X_train, y_train)

# .scoreで正解率を算出。
print("train score:",model.score(X_train,y_train))
print("test score:",model.score(X_test,y_test))
